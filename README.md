# ğŸ–¼ï¸ Image Caption Generator

An **Image Caption Generator** built using **Deep Learning** that combines **Convolutional Neural Networks (CNNs)** for image feature extraction and **Long Short-Term Memory (LSTM)** networks for generating natural language captions.

---

## ğŸ“Š Dataset
We use the **[Flickr8k Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k)**, which contains:
- 8,000 images
- Each image annotated with 5 different captions
- Diverse real-world objects, scenes, and actions

---

## ğŸ§  Model Architecture

The model integrates **two inputs**:
1. **Image Features** extracted using a pre-trained CNN (VGG16).
2. **Text Sequences** processed using Embedding + LSTM.

These two streams are merged and passed through Dense layers to generate the next word in the caption.

### Architecture Diagram
![Model Architecture](model.png)

**Key Layers:**
- **CNN Feature Extractor**: VGG16, output size 4096 â†’ Dense(256)
- **Text Encoder**: Embedding (256) + LSTM(256)
- **Fusion**: Add([CNN, LSTM])
- **Decoder**: Dense layers â†’ Vocabulary output (8,485 words)

---

## ğŸ“‚ Project Structure
image-caption-generator/

â”‚â”€â”€ captiongenerator.ipynb  #main notebook

â”‚â”€â”€ requirements.txt  #dependencies

â”‚â”€â”€ README.md  #project documentation

â”‚â”€â”€ model.png  #model architecture diagram

â”‚â”€â”€ data/  #pretrained model

---

## âš™ï¸ Installation
Clone this repository and install dependencies:

```bash
git clone https://github.com/your-username/image-caption-generator.git
cd image-caption-generator
pip install -r requirements.txt
```
---

## â–¶ï¸ Usage
You can either **train the model from scratch** or **use the pre-trained model** provided in the `data/` folder.
### Option 1: Run Full Training
Run the notebook:
```bash
jupyter notebook captiongenerator.ipynb
```
**Steps inside the notebook:**
1. Load and preprocess dataset captions.
2. Extract image features using VGG16.
3. Tokenize and encode text sequences.
4. Train CNN + LSTM model.
5. Generate captions for new images.

### Option 2: Use Pre-trained Model (Recommended)
We have provided the following files in the data/ folder:
- **features.pkl** â†’ pre-extracted image features
- **tokenizer.pkl** â†’ pre-trained tokenizer
- **model.h5** â†’ trained CNN-LSTM model
**Steps:**
1. Download the data/ folder contents.
2. Upload them into the notebook environment.
3. Run the importing, caption prediction and generation section in the notebook.
Generate captions for any custom image without retraining.

---

## ğŸ“¸ Example Output

Here are some example captions generated by the model:
### Example 1
![Boy in water](model.png)
### Example 2
![Dog catching frisbee](model.png)

